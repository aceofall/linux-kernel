# 2주차 Study Note (2013/05/04)

## Status
 - 인원: 27명
 - 장소: [성균관대학교 자연과학캠퍼스] (http://www.skku.edu/new_home/skku/campusinfo/location1.jsp)
 - 진도: [리눅스 커널 내부구조] (http://www.kangcom.com/sub/view.asp?sku=200809290003&mcd=571) (~p.150) <br  /> <br  />

## Notice
 - Zone, Node, Page Frame 의 내용을 다수의 인원이 이해하지 못하여, 해당 내용을 다시 한번 remind 수행 <br  /> <br  />



## Issue (Non-clear)
- [ ] **1. page frame의 경우 왜 4KB를 사용하는 것일까?** [[#5]](https://github.com/arm10c/linux-stable/issues/5)
    > [fragmentation] (https://en.wikipedia.org/wiki/Paging#Fragmentation) 문제이거나, 기존 관행적으로 사용해오던 암묵적인 rule 일 것이다.

  <br />
- [ ] **2. Slab, Slub, Slob의 의미가 교재를 통해 이해하기 어려움**
    
    > 오랜 동안 커널의 동적 메모리 할당자는 slab이었다. <br />
    > slab은 일반적인 환경에서 무난한 성능을 보여주었기 때문에 널리 사용되었지만 <br /> 
    > 메모리 자원에 상당한 제한을 받는 임베디드 환경에서나 매우 높은 확장성이 요구되는 서버 환경에서는 <br />
    > 용납하기 힘든 overhead를 지니고 있기 때문에 새로운 할당 알고리즘이 사용되고 있다. <br />

    > slob과 slub 할당자는 각각 2.6.16과 2.6.22 버전에서 추가된 것으로 <br />
    > 단 1KiB의 메모리도 아쉬운 제한적인 임베디드 환경에서는 slob을, <br />
    > 많은 수의 CPU와 (메모리) 노드로 구성된 서버 환경에서는 slub을 사용할 수 있다. <br />
    > (2.6.23 버전 이후로는 x86에서 기본 할당자로 slab 대신 slub이 설정되어 있다.)
   
    > 많은 커널 서적에서 이미 자세히 설명하고 있지만 <br />
    > 먼저 slab 할당자의 구조에 대해서 간략하게 살펴본 후에 slub과 slob에 대해서도 살펴보기로 하자.
    
    > 사실 slab 할당자는 일반적인 할당자가 아니라 특정한 객체(자료 구조)에 대해서만 할당을 수행하기 때문에 <br />
    > slab을 이용하기 위해서는 먼저 해당 slab이 어떤 객체를 다룰지 지정해 주어야 한다. <br />
    > 다르게 표현하면, slab이 해당 객체에 대한 캐시를 관리해주는 역할을 한다고 볼 수 있으므로 <br />
    > 먼저 객체에 대한 캐시를 만들어 두고 이 후에 필요할 때 캐시에서 객체를 할당받을 수 있는 것이다. <br />
    
    > 따라서 slab 할당자는 주어진 객체의 크기에 따라 커널의 buddy system으로부터 적당한 수의 페이지를 할당받고 <br />
    > 이를 하나로 묶어 여러 객체를 저장할 수 있는 단위로 관리하는데 이것이 바로 slab이다. <br />
    > 아래는 2개의 페이지로 구성된 하나의 slab을 나타낸 그림이다. <br /> 
    ![image](https://github.com/arm10c/linux-stable/blob/master/Reference/Breakdown/Figures/002_slab.png)
    
    > 각 slab은 실제 객체 이외에도 slab 자체에 대한 메타 정보를 포함하는 헤더와  <br />
    > slab 내의 모든 객체에 대한 할당 정보를 포함하는 테이블을 포함한다.  <br />
    > (이는 객체의 크기에 따라 slab 외부에 존재할 수도 있다.)  <br />
    
    > 이렇게 만들어진 slab은 객체가 사용된 정도에 따라 3가지로 구분되는데 (full, partial, empty)  <br />
    > 이는 또한 kmem_list3라는 구조체를 통해 (페이지 할당을 관리하는 단위인) 노드 별로 관리된다.  <br />
    
    > 하지만 이러한 slab은 성능 상의 이유로 메모리 할당 시 직접 접근하지 않고  <br />
    > 각 CPU 별로 할당된 array_cache 구조체를 통해 객체 단위로 캐시된다.  <br />
    
    > 이 외에도 kmem_list3에는 각 노드 별로 공유하는 array_cache 구조체를 두어  <br />
    > CPU 별 캐시 부족 시 공유 캐시로부터 CPU 별 캐시를 다시 채울 수 있도록 2차 캐시로 사용하며,  <br />
    > 메모리 부족 시 해당 CPU가 속한 노드가 아닌 외부 노드에서 할당한 객체가 있다면  <br />
    > 해당 객체를 해지할 때 이를 별도의 array_cache에 저장해 두었다가 해당 노드의 slab으로 돌려준다.  <br />
    > 이를 alien 캐시라고 하며 이는 각 kmem_list3 당 모든 노드 별로 존재한다.  <br />
    
    > 따라서 slab 할당자는 기본적으로 실제 할당 시 사용되는 slab (page) 외에도  <br />
    > 각 CPU 별 캐시 및 노드의 공유 캐시와 alien 캐시를 가지므로  <br />
    > 최대 NR_CPUS + MAX_NUMNODES + MAX_NUMNODES^2 개의 array_cache가 필요하다.  <br />

    > array_cache는 limit 개로 구성된 객체 포인터 배열을 가지고 있으며  <br />
    > 부족 시 batchcount 개의 객체 포인터를 공유 캐시 혹은 slab으로부터 채운다.  <br />
    > 공유 캐시는 최대 shared(factor) * batchcount 개의 객체 포인터를 유지하게 된다.  <br />
    > (즉, 공유 캐시를 구성하는 array_cache의 limit 값은 shared * batchcount이다.)  <br />
    > 이들은 /proc/slabinfo 파일을 통해 확인 및 변경이 가능하다. (tunables 항목)  <br />

    > 아래는 array_cache 구조체의 구조를 나타낸 그림이다.  <br />
